{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score,mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesPlaybookModel:\n",
    "    def __init__(self, deals_path, companies_path, tickets_path, mappings_path=None):\n",
    "        self.deals_path = deals_path\n",
    "        self.companies_path = companies_path\n",
    "        self.tickets_path = tickets_path\n",
    "        self.mappings_path = mappings_path\n",
    "        \n",
    "        # Instance variables for data\n",
    "        self.deals = None\n",
    "        self.companies = None\n",
    "        self.tickets = None\n",
    "        self.merged_df = None\n",
    "        self.mappings = None\n",
    "        \n",
    "        # Model components\n",
    "        self.classifier = None\n",
    "        self.regressor = None\n",
    "        self.preprocessing_pipeline = None\n",
    "        self.cluster_model = None\n",
    "        \n",
    "        # Load the data\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load datasets and mappings\"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "        \n",
    "        # Load CSV files\n",
    "        self.deals = pd.read_csv(self.deals_path)\n",
    "        self.companies = pd.read_csv(self.companies_path)\n",
    "        self.tickets = pd.read_csv(self.tickets_path)\n",
    "        \n",
    "        print(f\"Loaded deals: {len(self.deals)} records\")\n",
    "        print(f\"Loaded companies: {len(self.companies)} records\")\n",
    "        print(f\"Loaded tickets: {len(self.tickets)} records\")\n",
    "        \n",
    "        # Load mappings if provided\n",
    "        if self.mappings_path:\n",
    "            with open(self.mappings_path, 'r') as f:\n",
    "                self.mappings = json.load(f)\n",
    "            print(\"Loaded mappings data\")\n",
    "    \n",
    "    def process_and_merge_data(self):\n",
    "        \"\"\"Clean, transform and merge datasets\"\"\"\n",
    "        print(\"Processing and merging data...\")\n",
    "        \n",
    "        # ----- PREPARE COMPANIES DATA -----\n",
    "        # Filter out columns with more than 80% missing values\n",
    "        valid_company_cols = self.companies.columns[self.companies.isnull().mean() < 0.8].tolist()\n",
    "        company_subset = self.companies[valid_company_cols]\n",
    "        \n",
    "        # Ensure ID columns are string type for consistent merging\n",
    "        if 'Record ID' in company_subset.columns:\n",
    "            company_subset['Record ID'] = company_subset['Record ID'].astype(str)\n",
    "        \n",
    "        # ----- PREPARE DEALS DATA -----\n",
    "        # Convert date columns to datetime\n",
    "        date_columns = [col for col in self.deals.columns if 'Date' in col]\n",
    "        for col in date_columns:\n",
    "            if col in self.deals.columns:\n",
    "                self.deals[col] = pd.to_datetime(self.deals[col], errors='coerce')\n",
    "        \n",
    "        # Create time_to_close_days as target variable\n",
    "        if 'Create Date' in self.deals.columns and 'Close Date' in self.deals.columns:\n",
    "            self.deals['time_to_close_days'] = (self.deals['Close Date'] - self.deals['Create Date']).dt.days\n",
    "            \n",
    "            # Filter out invalid or extreme values\n",
    "            self.deals = self.deals.dropna(subset=['time_to_close_days'])\n",
    "            self.deals = self.deals[(self.deals['time_to_close_days'] > 0) & (self.deals['time_to_close_days'] <= 365)]\n",
    "        \n",
    "        # Ensure deal ID columns are string type\n",
    "        if 'Record ID' in self.deals.columns:\n",
    "            self.deals['Record ID'] = self.deals['Record ID'].astype(str)\n",
    "        \n",
    "        # ----- MERGE TICKETS WITH DEALS -----\n",
    "        # Count tickets per deal if possible\n",
    "        if 'Associated Deal' in self.tickets.columns and 'Record ID' in self.deals.columns:\n",
    "            # Create mapping for tickets to deals\n",
    "            ticket_counts = self.tickets['Associated Deal'].value_counts().reset_index()\n",
    "            ticket_counts.columns = ['Record ID', 'ticket_count_per_deal']\n",
    "            \n",
    "            # Convert to string for consistent merging\n",
    "            ticket_counts['Record ID'] = ticket_counts['Record ID'].astype(str)\n",
    "            \n",
    "            # Merge ticket counts with deals\n",
    "            self.deals = self.deals.merge(ticket_counts, on='Record ID', how='left')\n",
    "            self.deals['ticket_count_per_deal'] = self.deals['ticket_count_per_deal'].fillna(0)\n",
    "        else:\n",
    "            # Create a default column if no mapping is possible\n",
    "            self.deals['ticket_count_per_deal'] = 0\n",
    "            \n",
    "        # ----- PROCESS TRAINING FLAGS -----\n",
    "        # Extract training data from tickets\n",
    "        if 'Training: General Overview' in self.tickets.columns:\n",
    "            training_columns = [col for col in self.tickets.columns if 'Training:' in col]\n",
    "            \n",
    "            # Create a flag for any training provided\n",
    "            if len(training_columns) > 0:\n",
    "                self.tickets['Any Training Provided'] = self.tickets[training_columns].notna().any(axis=1)\n",
    "                \n",
    "                # Create mapping to merge with deals\n",
    "                if 'Associated Company (Primary)' in self.tickets.columns:\n",
    "                    training_per_company = self.tickets[['Associated Company (Primary)', 'Any Training Provided']].copy()\n",
    "                    training_per_company = training_per_company.drop_duplicates('Associated Company (Primary)')\n",
    "                    \n",
    "                    # Merge training data if possible\n",
    "                    if 'Associated Company (Primary)' in self.deals.columns:\n",
    "                        self.deals = self.deals.merge(\n",
    "                            training_per_company, \n",
    "                            on='Associated Company (Primary)', \n",
    "                            how='left'\n",
    "                        )\n",
    "                        self.deals['Any Training Provided'] = self.deals['Any Training Provided'].fillna(False)\n",
    "        \n",
    "        # ----- MERGE DEALS WITH COMPANIES -----\n",
    "        # Check if we have mappings to use\n",
    "        if self.mappings and 'CompanyToDeals' in self.mappings:\n",
    "            # Create a mapping dictionary: Deal ID -> Company ID\n",
    "            deal_to_company = {}\n",
    "            for company_id, deal_ids in self.mappings['CompanyToDeals'].items():\n",
    "                for deal_id in deal_ids:\n",
    "                    deal_to_company[deal_id] = company_id\n",
    "            \n",
    "            # Add company ID to each deal based on mapping\n",
    "            self.deals['Company Record ID'] = self.deals['Record ID'].map(deal_to_company)\n",
    "            \n",
    "            # Merge deals with companies\n",
    "            self.merged_df = self.deals.merge(\n",
    "                company_subset,\n",
    "                left_on='Company Record ID',\n",
    "                right_on='Record ID',\n",
    "                how='left',\n",
    "                suffixes=('_deal', '_company')\n",
    "            )\n",
    "        else:\n",
    "            # Direct merge on Associated Company if available\n",
    "            if 'Associated Company (Primary)' in self.deals.columns and 'Company name' in company_subset.columns:\n",
    "                self.merged_df = self.deals.merge(\n",
    "                    company_subset,\n",
    "                    left_on='Associated Company (Primary)',\n",
    "                    right_on='Company name',\n",
    "                    how='left'\n",
    "                )\n",
    "            else:\n",
    "                # If no proper join is possible, just use the deals\n",
    "                self.merged_df = self.deals.copy()\n",
    "                print(\"Warning: Could not properly merge deals with companies.\")\n",
    "        \n",
    "        # ----- FEATURE ENGINEERING -----\n",
    "        # Create time-based features\n",
    "        # Ensure Create Date is datetime\n",
    "            self.deals['Create Date'] = pd.to_datetime(self.deals['Create Date'], errors='coerce')\n",
    "\n",
    "            # Create time-based features\n",
    "            self.deals['Create Month'] = self.deals['Create Date'].dt.month\n",
    "            self.deals['Create Quarter'] = self.deals['Create Date'].dt.quarter\n",
    "            self.deals['Create Year'] = self.deals['Create Date'].dt.year\n",
    "            self.deals['Days Since Creation'] = (pd.Timestamp.today() - self.deals['Create Date']).dt.days\n",
    "\n",
    "        if 'Create Date' in self.merged_df.columns:\n",
    "            self.merged_df['Create Month'] = self.merged_df['Create Date'].dt.month\n",
    "            self.merged_df['Create Quarter'] = self.merged_df['Create Date'].dt.quarter\n",
    "            self.merged_df['Create Year'] = self.merged_df['Create Date'].dt.year\n",
    "            \n",
    "            # Days since creation\n",
    "            latest_date = self.merged_df['Create Date'].max()\n",
    "            self.merged_df['Days Since Creation'] = (latest_date - self.merged_df['Create Date']).dt.days\n",
    "        \n",
    "        # Create deal duration categories\n",
    "        if 'time_to_close_days' in self.merged_df.columns:\n",
    "            self.merged_df['Duration Category'] = pd.cut(\n",
    "                self.merged_df['time_to_close_days'],\n",
    "                bins=[0, 30, 90, 180, 365],\n",
    "                labels=['Short', 'Medium', 'Long', 'Very Long']\n",
    "            )\n",
    "        \n",
    "        # Create win/loss target feature\n",
    "        if 'Is Closed Won' in self.merged_df.columns:\n",
    "            self.merged_df['Closed_Won_Label'] = self.merged_df['Is Closed Won'].map({True: 'Closed Won', False: 'Closed Lost'})\n",
    "        elif 'Deal Stage' in self.merged_df.columns:\n",
    "            self.merged_df['Closed_Won_Label'] = self.merged_df['Deal Stage'].apply(\n",
    "                lambda x: 'Closed Won' if 'Won' in str(x) else 'Closed Lost'\n",
    "            )\n",
    "        \n",
    "        print(f\"Processed and merged data: {len(self.merged_df)} records with {len(self.merged_df.columns)} features\")\n",
    "        return self.merged_df\n",
    "    \n",
    "    def perform_exploratory_analysis(self, output_dir=None):\n",
    "        \"\"\"Perform exploratory analysis and generate insights\"\"\"\n",
    "        print(\"Performing exploratory analysis...\")\n",
    "        \n",
    "        if self.merged_df is None:\n",
    "            print(\"No merged data available. Call process_and_merge_data() first.\")\n",
    "            return\n",
    "        \n",
    "        # Create output directory if provided\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # ----- ANALYZE MISSING VALUES -----\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        missing_counts = self.merged_df.isnull().sum()\n",
    "        missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "        \n",
    "        if len(missing_counts) > 0:\n",
    "            missing_counts.head(20).plot(kind='bar')\n",
    "            plt.title('Missing Values Per Column')\n",
    "            plt.ylabel('Number of Missing Values')\n",
    "            plt.xlabel('Columns')\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if output_dir:\n",
    "                plt.savefig(f\"{output_dir}/missing_values.png\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "        \n",
    "        # ----- ANALYZE WIN RATES -----\n",
    "        if 'Closed_Won_Label' in self.merged_df.columns:\n",
    "            # Win rate by ticket volume\n",
    "            if 'ticket_count_per_deal' in self.merged_df.columns:\n",
    "                # Define ticket volume cohort buckets\n",
    "                def bucket_ticket_count(n):\n",
    "                    if n == 0:\n",
    "                        return \"0 Tickets\"\n",
    "                    elif n <= 2:\n",
    "                        return \"1-2 Tickets\"\n",
    "                    else:\n",
    "                        return \"3+ Tickets\"\n",
    "                \n",
    "                self.merged_df['Ticket Volume Cohort'] = self.merged_df['ticket_count_per_deal'].apply(bucket_ticket_count)\n",
    "                \n",
    "                # Analyze success rate by ticket volume cohort\n",
    "                ticket_cohort_props = (\n",
    "                    self.merged_df.groupby('Ticket Volume Cohort')['Closed_Won_Label']\n",
    "                    .value_counts(normalize=True)\n",
    "                    .unstack()\n",
    "                    .fillna(0)\n",
    "                )\n",
    "                \n",
    "                plt.figure(figsize=(8, 5))\n",
    "                ticket_cohort_props.plot(kind='bar', stacked=True)\n",
    "                plt.title('Deal Success Rate by Ticket Volume Cohort')\n",
    "                plt.ylabel('Proportion')\n",
    "                plt.xlabel('Ticket Volume Cohort')\n",
    "                plt.xticks(rotation=0)\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if output_dir:\n",
    "                    plt.savefig(f\"{output_dir}/success_by_ticket_volume.png\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "            \n",
    "            # Win rate by training exposure\n",
    "            if 'Any Training Provided' in self.merged_df.columns:\n",
    "                cohort_props = (\n",
    "                    self.merged_df.groupby('Any Training Provided')['Closed_Won_Label']\n",
    "                    .value_counts(normalize=True)\n",
    "                    .unstack()\n",
    "                    .fillna(0)\n",
    "                )\n",
    "                \n",
    "                plt.figure(figsize=(8, 5))\n",
    "                cohort_props.plot(kind='bar', stacked=True)\n",
    "                plt.title('Deal Success Rate by Training Exposure')\n",
    "                plt.ylabel('Proportion')\n",
    "                plt.xlabel('Any Training Provided')\n",
    "                plt.xticks([0, 1], ['No Training', 'Training Provided'], rotation=0)\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if output_dir:\n",
    "                    plt.savefig(f\"{output_dir}/success_by_training.png\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "        \n",
    "        # ----- ANALYZE TIME TO CLOSE -----\n",
    "        if 'time_to_close_days' in self.merged_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(self.merged_df['time_to_close_days'], bins=30)\n",
    "            plt.title('Distribution of Time to Close (Days)')\n",
    "            plt.xlabel('Days to Close')\n",
    "            plt.ylabel('Count')\n",
    "            \n",
    "            if output_dir:\n",
    "                plt.savefig(f\"{output_dir}/time_to_close_distribution.png\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "            \n",
    "            # Time to close by deal outcome\n",
    "            if 'Closed_Won_Label' in self.merged_df.columns:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.boxplot(x='Closed_Won_Label', y='time_to_close_days', data=self.merged_df)\n",
    "                plt.title('Time to Close by Deal Outcome')\n",
    "                plt.ylabel('Days to Close')\n",
    "                plt.xlabel('Deal Outcome')\n",
    "                \n",
    "                if output_dir:\n",
    "                    plt.savefig(f\"{output_dir}/time_to_close_by_outcome.png\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "        \n",
    "        return \"Completed exploratory analysis\"\n",
    "    \n",
    "    def cluster_companies(self, n_clusters=5):\n",
    "        print(f\"Clustering companies into {n_clusters} segments...\")\n",
    "        \n",
    "        if self.merged_df is None or len(self.merged_df) == 0:\n",
    "            print(\"No merged data available. Call process_and_merge_data() first.\")\n",
    "            return None\n",
    "            \n",
    "        # Make a copy to avoid modifying the original\n",
    "        cluster_df = self.merged_df.copy()\n",
    "        \n",
    "        # Create time-based features if they don't exist\n",
    "        if 'Create Date' in cluster_df.columns:\n",
    "            try:\n",
    "                # Convert to datetime first\n",
    "                cluster_df['Create Date'] = pd.to_datetime(cluster_df['Create Date'], errors='coerce')\n",
    "                \n",
    "                # Add derived date columns\n",
    "                cluster_df['Create Month'] = cluster_df['Create Date'].dt.month\n",
    "                cluster_df['Create Quarter'] = cluster_df['Create Date'].dt.quarter\n",
    "                cluster_df['Create Year'] = cluster_df['Create Date'].dt.year\n",
    "                \n",
    "                # Calculate days since creation using the current date\n",
    "                latest_date = cluster_df['Create Date'].max()\n",
    "                cluster_df['Days Since Creation'] = (latest_date - cluster_df['Create Date']).dt.days\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process date columns: {e}\")\n",
    "        \n",
    "        # Define potential clustering features\n",
    "        potential_categorical = ['Deal Stage', 'Forecast category', 'Duration Category',\n",
    "                                'Pipeline', 'Industry', 'Company Size']\n",
    "        potential_numerical = ['Amount', 'Deal probability', 'Weighted amount', 'Forecast amount',\n",
    "                            'Create Month', 'Create Quarter', 'Create Year', 'Days Since Creation',\n",
    "                            'Number of Employees', 'Annual Revenue', 'ticket_count_per_deal']\n",
    "        \n",
    "        # Filter to only use columns that actually exist in the dataframe\n",
    "        categorical = [col for col in potential_categorical if col in cluster_df.columns]\n",
    "        numerical = [col for col in potential_numerical if col in cluster_df.columns]\n",
    "        \n",
    "        # Make sure we have at least some features to work with\n",
    "        if len(categorical) == 0 and len(numerical) == 0:\n",
    "            print(\"Error: No valid clustering features found in the dataset\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Using categorical features: {categorical}\")\n",
    "        print(f\"Using numerical features: {numerical}\")\n",
    "        \n",
    "        # Clean data for clustering (drop rows with missing values in selected features)\n",
    "        selected_features = categorical + numerical\n",
    "        analysis_df = cluster_df[selected_features].copy()\n",
    "        \n",
    "        # Handle missing values for numerical columns\n",
    "        for col in numerical:\n",
    "            analysis_df[col] = analysis_df[col].fillna(analysis_df[col].median())\n",
    "        \n",
    "        # Handle categorical features by converting to string and filling missing values\n",
    "        # This is key to fix the categorical error\n",
    "        for col in categorical:\n",
    "            # Convert to string first to avoid categorical issues\n",
    "            analysis_df[col] = analysis_df[col].astype(str)\n",
    "            # Replace 'nan' strings with 'Unknown'\n",
    "            analysis_df[col] = analysis_df[col].replace('nan', 'Unknown')\n",
    "        \n",
    "        try:\n",
    "            # Create preprocessing pipeline\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "            \n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ])\n",
    "            \n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numeric_transformer, numerical),\n",
    "                    ('cat', categorical_transformer, categorical)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            X_cluster_ready = preprocessor.fit_transform(analysis_df)\n",
    "            \n",
    "            # Apply KMeans clustering\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(X_cluster_ready)\n",
    "            \n",
    "            # Store the cluster model and preprocessor\n",
    "            self.cluster_model = kmeans\n",
    "            self.preprocessing_pipeline = preprocessor\n",
    "            \n",
    "            # Add cluster labels back to analysis dataframe\n",
    "            analysis_df['Cluster'] = cluster_labels\n",
    "            self.analysis_df = analysis_df  # Store for later use\n",
    "            \n",
    "            # Create a summary of cluster characteristics\n",
    "            # For numeric features, use mean\n",
    "            if numerical:\n",
    "                cluster_summary = analysis_df.groupby('Cluster')[numerical].mean().round(2)\n",
    "            else:\n",
    "                # Create empty DataFrame if no numerical features\n",
    "                cluster_summary = pd.DataFrame(index=range(n_clusters))\n",
    "                cluster_summary.index.name = 'Cluster'\n",
    "            \n",
    "            # Add categorical feature distributions\n",
    "            for cat_col in categorical:\n",
    "                # Get distribution of categories within each cluster\n",
    "                cat_dist = pd.crosstab(\n",
    "                    analysis_df['Cluster'],\n",
    "                    analysis_df[cat_col],\n",
    "                    normalize='index'\n",
    "                ).round(2)\n",
    "                \n",
    "                # Find the dominant category for each cluster\n",
    "                if not cat_dist.empty:\n",
    "                    dominant_cats = cat_dist.idxmax(axis=1).to_frame(f'Top {cat_col}')\n",
    "                    \n",
    "                    # Add to summary\n",
    "                    cluster_summary = pd.merge(\n",
    "                        cluster_summary,\n",
    "                        dominant_cats,\n",
    "                        left_index=True,\n",
    "                        right_index=True,\n",
    "                    )\n",
    "            \n",
    "            # Add count of companies in each cluster\n",
    "            cluster_counts = analysis_df['Cluster'].value_counts().sort_index().to_frame('Company Count')\n",
    "            cluster_summary = pd.merge(\n",
    "                cluster_summary,\n",
    "                cluster_counts,\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "            )\n",
    "            \n",
    "            # Calculate cluster success metrics if available\n",
    "            if 'Is Closed Won' in cluster_df.columns:\n",
    "                # Create a mapping from analysis_df to cluster_df\n",
    "                cluster_mapping = analysis_df['Cluster'].to_dict()\n",
    "                \n",
    "                # Add cluster labels to original dataframe where possible\n",
    "                cluster_df.loc[analysis_df.index, 'Cluster'] = cluster_df.loc[analysis_df.index].index.map(cluster_mapping)\n",
    "                \n",
    "                # Calculate win rate by cluster\n",
    "                win_rate = cluster_df.groupby('Cluster')['Is Closed Won'].mean().round(3) * 100\n",
    "                win_rate = win_rate.to_frame('Win Rate %')\n",
    "                \n",
    "                # Add to summary\n",
    "                cluster_summary = pd.merge(\n",
    "                    cluster_summary,\n",
    "                    win_rate,\n",
    "                    left_index=True,\n",
    "                    right_index=True,\n",
    "                    how='left'  # Use left join in case some clusters have no win/loss data\n",
    "                )\n",
    "            \n",
    "            # If time to close days is available, add average per cluster\n",
    "            if 'time_to_close_days' in cluster_df.columns:\n",
    "                avg_time = cluster_df.groupby('Cluster')['time_to_close_days'].mean().round(1)\n",
    "                avg_time = avg_time.to_frame('Avg Days to Close')\n",
    "                \n",
    "                # Add to summary\n",
    "                cluster_summary = pd.merge(\n",
    "                    cluster_summary,\n",
    "                    avg_time,\n",
    "                    left_index=True,\n",
    "                    right_index=True,\n",
    "                    how='left'  # Use left join in case some clusters have no time data\n",
    "                )\n",
    "                \n",
    "            print(\"Clustering completed successfully\")\n",
    "            return cluster_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in clustering process: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def visualize_clusters(self, n_clusters=5):\n",
    "        \n",
    "        if not hasattr(self, 'analysis_df') or self.analysis_df is None or 'Cluster' not in self.analysis_df.columns:\n",
    "            return {\"error\": \"No clustering data available. Run clustering first.\"}\n",
    "        \n",
    "        result = {}\n",
    "        \n",
    "        try:\n",
    "            # Check which columns are available for visualization\n",
    "            # Remove date columns from visualization if they don't exist in the analysis_df\n",
    "            date_columns = ['Create Month', 'Create Quarter', 'Create Year', 'Days Since Creation']\n",
    "            \n",
    "            # Get all numeric columns except Cluster for PCA\n",
    "            numeric_cols = [col for col in self.analysis_df.columns \n",
    "                        if col != 'Cluster' \n",
    "                        and pd.api.types.is_numeric_dtype(self.analysis_df[col])]\n",
    "            \n",
    "            # Remove any date columns that aren't in the dataframe\n",
    "            numeric_cols = [col for col in numeric_cols if col not in date_columns or col in self.analysis_df.columns]\n",
    "            \n",
    "            if len(numeric_cols) >= 2:  # Need at least 2 dimensions for PCA\n",
    "                # Create PCA for visualization\n",
    "                pca = PCA(n_components=2)\n",
    "                \n",
    "                # Create scatter plot of clusters\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                \n",
    "                # Apply PCA to numeric features\n",
    "                pca_result = pca.fit_transform(self.analysis_df[numeric_cols])\n",
    "                \n",
    "                # Create scatter plot\n",
    "                scatter = plt.scatter(\n",
    "                    pca_result[:, 0], \n",
    "                    pca_result[:, 1], \n",
    "                    c=self.analysis_df['Cluster'], \n",
    "                    cmap='viridis', \n",
    "                    alpha=0.6, \n",
    "                    s=50\n",
    "                )\n",
    "                \n",
    "                # Add cluster centers if we have the model\n",
    "                if self.cluster_model is not None:\n",
    "                    # Transform cluster centers through PCA\n",
    "                    centers_pca = pca.transform(self.preprocessing_pipeline.named_transformers_['num'].inverse_transform(\n",
    "                        self.cluster_model.cluster_centers_[:, :len(numeric_cols)]\n",
    "                    ))\n",
    "                    \n",
    "                    # Add cluster centers to plot\n",
    "                    plt.scatter(\n",
    "                        centers_pca[:, 0], \n",
    "                        centers_pca[:, 1], \n",
    "                        c=range(n_clusters), \n",
    "                        cmap='viridis', \n",
    "                        marker='x', \n",
    "                        s=200, \n",
    "                        linewidths=3\n",
    "                    )\n",
    "                \n",
    "                # Add labels and legend\n",
    "                plt.title('Customer Segments (PCA Projection)', fontsize=14)\n",
    "                plt.xlabel(f'Principal Component 1', fontsize=12)\n",
    "                plt.ylabel(f'Principal Component 2', fontsize=12)\n",
    "                plt.colorbar(scatter, label='Cluster')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Add variance explained\n",
    "                explained_variance = pca.explained_variance_ratio_\n",
    "                plt.figtext(\n",
    "                    0.02, 0.02, \n",
    "                    f'Explained variance: PC1={explained_variance[0]:.2f}, PC2={explained_variance[1]:.2f}',\n",
    "                    fontsize=10\n",
    "                )\n",
    "                \n",
    "                result['scatter_plot'] = plt.gcf()\n",
    "                plt.close()\n",
    "                \n",
    "            else:\n",
    "                result['error'] = \"Insufficient numeric data for visualization\"\n",
    "                \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error performing clustering visualization: {str(e)}\"}\n",
    "        \n",
    "    def train_deal_outcome_model(self):\n",
    "        \"\"\"Train a model to predict deal outcome (win/loss)\"\"\"\n",
    "        if self.merged_df is None or len(self.merged_df) == 0:\n",
    "            print(\"No data available for training\")\n",
    "            return None\n",
    "        \n",
    "        # Make a copy of the data for training\n",
    "        train_df = self.merged_df.copy()\n",
    "        \n",
    "        # Ensure we have the target variable\n",
    "        if 'Is Closed Won' not in train_df.columns:\n",
    "            print(\"Error: Target variable 'Is Closed Won' not found in data\")\n",
    "            return None\n",
    "        \n",
    "        # Process date columns before training\n",
    "        if 'Create Date' in train_df.columns:\n",
    "            try:\n",
    "                # Convert to datetime\n",
    "                train_df['Create Date'] = pd.to_datetime(train_df['Create Date'], errors='coerce')\n",
    "                \n",
    "                # Explicitly add date-derived columns\n",
    "                train_df['Create Month'] = train_df['Create Date'].dt.month\n",
    "                train_df['Create Quarter'] = train_df['Create Date'].dt.quarter\n",
    "                train_df['Create Year'] = train_df['Create Date'].dt.year\n",
    "                \n",
    "                # Calculate days since creation\n",
    "                latest_date = train_df['Create Date'].max()\n",
    "                train_df['Days Since Creation'] = (latest_date - train_df['Create Date']).dt.days\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process date columns: {e}\")\n",
    "        \n",
    "        # Define features for training\n",
    "        categorical_features = ['Deal Stage', 'Forecast category', 'Pipeline', 'Industry', 'Company Size']\n",
    "        numerical_features = ['Amount', 'Deal probability', 'Weighted amount', 'Forecast amount',\n",
    "                            'Number of Employees', 'Annual Revenue', 'ticket_count_per_deal']\n",
    "        \n",
    "        # Add date features if they exist\n",
    "        date_features = ['Create Month', 'Create Quarter', 'Create Year', 'Days Since Creation']\n",
    "        for feature in date_features:\n",
    "            if feature in train_df.columns:\n",
    "                numerical_features.append(feature)\n",
    "        \n",
    "        # Filter to columns that exist in the dataframe\n",
    "        categorical_features = [col for col in categorical_features if col in train_df.columns]\n",
    "        numerical_features = [col for col in numerical_features if col in train_df.columns]\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = categorical_features + numerical_features\n",
    "        \n",
    "        if not all_features:\n",
    "            print(\"Error: No valid features for training\")\n",
    "            return None\n",
    "        \n",
    "        # Handle missing values\n",
    "        for col in numerical_features:\n",
    "            train_df[col] = train_df[col].fillna(train_df[col].median())\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            train_df[col] = train_df[col].astype(str).fillna('Unknown')\n",
    "        \n",
    "        # Define X and y\n",
    "        X = train_df[all_features]\n",
    "        y = train_df['Is Closed Won']\n",
    "        \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numerical_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Store the model\n",
    "        self.classifier = model\n",
    "        \n",
    "        # Calculate model performance\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'f1_score': f1_score(y, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        print(f\"Deal outcome model trained: Accuracy = {results['accuracy']:.2f}, F1 = {results['f1_score']:.2f}\")\n",
    "        return results\n",
    "\n",
    "    def train_time_to_close_model(self):\n",
    "        \n",
    "        if self.merged_df is None or len(self.merged_df) == 0:\n",
    "            print(\"No data available for training\")\n",
    "            return None\n",
    "        \n",
    "        # Make a copy of the data for training\n",
    "        train_df = self.merged_df.copy()\n",
    "        \n",
    "        # Ensure we have the target variable\n",
    "        if 'time_to_close_days' not in train_df.columns:\n",
    "            print(\"Error: Target variable 'time_to_close_days' not found in data\")\n",
    "            return None\n",
    "        \n",
    "        # Process date columns before training\n",
    "        if 'Create Date' in train_df.columns:\n",
    "            try:\n",
    "                # Convert to datetime\n",
    "                train_df['Create Date'] = pd.to_datetime(train_df['Create Date'], errors='coerce')\n",
    "                \n",
    "                # Explicitly add date-derived columns\n",
    "                train_df['Create Month'] = train_df['Create Date'].dt.month\n",
    "                train_df['Create Quarter'] = train_df['Create Date'].dt.quarter\n",
    "                train_df['Create Year'] = train_df['Create Date'].dt.year\n",
    "                \n",
    "                # Calculate days since creation\n",
    "                latest_date = train_df['Create Date'].max()\n",
    "                train_df['Days Since Creation'] = (latest_date - train_df['Create Date']).dt.days\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process date columns: {e}\")\n",
    "        \n",
    "        # Define features for training\n",
    "        categorical_features = ['Deal Stage', 'Forecast category', 'Pipeline', 'Industry', 'Company Size']\n",
    "        numerical_features = ['Amount', 'Deal probability', 'Weighted amount', 'Forecast amount',\n",
    "                            'Number of Employees', 'Annual Revenue', 'ticket_count_per_deal']\n",
    "        \n",
    "        # Add date features if they exist\n",
    "        date_features = ['Create Month', 'Create Quarter', 'Create Year', 'Days Since Creation']\n",
    "        for feature in date_features:\n",
    "            if feature in train_df.columns:\n",
    "                numerical_features.append(feature)\n",
    "        \n",
    "        # Filter to columns that exist in the dataframe\n",
    "        categorical_features = [col for col in categorical_features if col in train_df.columns]\n",
    "        numerical_features = [col for col in numerical_features if col in train_df.columns]\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = categorical_features + numerical_features\n",
    "        \n",
    "        if not all_features:\n",
    "            print(\"Error: No valid features for training\")\n",
    "            return None\n",
    "        \n",
    "        # Handle missing values\n",
    "        for col in numerical_features:\n",
    "            train_df[col] = train_df[col].fillna(train_df[col].median())\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            train_df[col] = train_df[col].astype(str).fillna('Unknown')\n",
    "        \n",
    "        # Define X and y - logarithmic transformation of target for better regression\n",
    "        X = train_df[all_features]\n",
    "        y = np.log1p(train_df['time_to_close_days'])  # Log transform to normalize\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numerical_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Store the model\n",
    "        self.regressor = model\n",
    "        \n",
    "        # Calculate model performance\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        results = {\n",
    "            'rmse': np.sqrt(mean_squared_error(y, y_pred)),\n",
    "            'r2_score': r2_score(y, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"Time to close model trained: RMSE = {results['rmse']:.2f}, R² = {results['r2_score']:.2f}\")\n",
    "        return results\n",
    "    \n",
    "    def predict_new_deal(self, deal_data):  \n",
    "        if self.classifier is None or self.regressor is None:\n",
    "            print(\"Models not trained. Call train_deal_outcome_model() and train_time_to_close_model() first.\")\n",
    "            return None\n",
    "        \n",
    "        # Create a DataFrame from the input data\n",
    "        deal_df = pd.DataFrame([deal_data])\n",
    "        \n",
    "        # Process the input similar to training data\n",
    "        # Generate features using same transformations as training\n",
    "        if 'Create Date' in deal_df.columns:\n",
    "            try:\n",
    "                deal_df['Create Date'] = pd.to_datetime(deal_df['Create Date'])\n",
    "                deal_df['Create Month'] = deal_df['Create Date'].dt.month\n",
    "                deal_df['Create Quarter'] = deal_df['Create Date'].dt.quarter\n",
    "                deal_df['Create Year'] = deal_df['Create Date'].dt.year\n",
    "                deal_df['Days Since Creation'] = (datetime.now() - deal_df['Create Date']).dt.days\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process date features: {e}\")\n",
    "        \n",
    "        # Add default values for missing columns\n",
    "        expected_columns = [\n",
    "            'Amount', 'Deal probability', 'Weighted amount', 'Forecast amount',\n",
    "            'Create Date', 'Pipeline', 'Deal Stage', 'Forecast category',\n",
    "            'Industry', 'Number of Employees', 'Annual Revenue', 'ticket_count_per_deal'\n",
    "        ]\n",
    "        \n",
    "        # Add any missing columns with default values\n",
    "        for col in expected_columns:\n",
    "            if col not in deal_df.columns:\n",
    "                if col in ['Amount', 'Deal probability', 'Weighted amount', 'Forecast amount', \n",
    "                        'Number of Employees', 'Annual Revenue', 'ticket_count_per_deal']:\n",
    "                    deal_df[col] = 0  # Default numeric value\n",
    "                else:\n",
    "                    deal_df[col] = 'Unknown'  # Default categorical value\n",
    "        \n",
    "        # Make predictions\n",
    "        try:\n",
    "            win_prob = self.classifier.predict_proba(deal_df)[:, 1][0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting win probability: {e}\")\n",
    "            win_prob = 0.5  # Default value\n",
    "        \n",
    "        try:\n",
    "            close_time_log = self.regressor.predict(deal_df)[0]\n",
    "            close_time_days = np.expm1(close_time_log)\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting closing time: {e}\")\n",
    "            close_time_days = 30  # Default value\n",
    "        \n",
    "        return {\n",
    "            'win_probability': win_prob,\n",
    "            'estimated_days_to_close': close_time_days,\n",
    "            'estimated_close_date': (datetime.now() + pd.Timedelta(days=close_time_days)).strftime('%Y-%m-%d')\n",
    "        }\n",
    "    \n",
    "    def get_feature_importances(self, model_type='classifier'):\n",
    "        if model_type == 'classifier' and self.classifier is None:\n",
    "            return None\n",
    "        if model_type == 'regressor' and self.regressor is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            if model_type == 'classifier':\n",
    "                model = self.classifier\n",
    "            else:\n",
    "                model = self.regressor\n",
    "            \n",
    "            # Check if the model is a Pipeline\n",
    "            if hasattr(model, 'named_steps') and 'classifier' in model.named_steps:\n",
    "                estimator = model.named_steps['classifier']\n",
    "            elif hasattr(model, 'named_steps') and 'regressor' in model.named_steps:\n",
    "                estimator = model.named_steps['regressor']\n",
    "            else:\n",
    "                estimator = model\n",
    "            \n",
    "            # Get feature names\n",
    "            if hasattr(model, 'named_steps') and 'preprocessor' in model.named_steps:\n",
    "                preprocessor = model.named_steps['preprocessor']\n",
    "                \n",
    "                # Get transformed feature names\n",
    "                if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "                    feature_names = preprocessor.get_feature_names_out()\n",
    "                else:\n",
    "                    # If get_feature_names_out is not available, use generic feature names\n",
    "                    feature_names = [f'feature_{i}' for i in range(len(estimator.feature_importances_))]\n",
    "            else:\n",
    "                feature_names = [f'feature_{i}' for i in range(len(estimator.feature_importances_))]\n",
    "            \n",
    "            # Get feature importances\n",
    "            if hasattr(estimator, 'feature_importances_'):\n",
    "                importances = estimator.feature_importances_\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            # Create DataFrame\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            })\n",
    "            \n",
    "            # Sort by importance\n",
    "            feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "            \n",
    "            return feature_importance_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting feature importances: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_models(self, output_dir=\"models\"):    \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        if hasattr(self, 'classifier') and self.classifier:\n",
    "            with open(f\"{output_dir}/outcome_classifier.pkl\", 'wb') as f:\n",
    "                pickle.dump(self.classifier, f)\n",
    "        \n",
    "        if hasattr(self, 'regressor') and self.regressor:\n",
    "            with open(f\"{output_dir}/time_regressor.pkl\", 'wb') as f:\n",
    "                pickle.dump(self.regressor, f)\n",
    "        \n",
    "        if hasattr(self, 'cluster_model') and self.cluster_model:\n",
    "            with open(f\"{output_dir}/cluster_model.pkl\", 'wb') as f:\n",
    "                pickle.dump(self.cluster_model, f)\n",
    "        \n",
    "        if hasattr(self, 'preprocessing_pipeline') and self.preprocessing_pipeline:\n",
    "            with open(f\"{output_dir}/preprocessing_pipeline.pkl\", 'wb') as f:\n",
    "                pickle.dump(self.preprocessing_pipeline, f)\n",
    "        \n",
    "        print(f\"Models saved to {output_dir}\")\n",
    "        return True\n",
    "\n",
    "    def load_models(self, input_dir=\"models\"):       \n",
    "        try:\n",
    "            # Check if directory exists\n",
    "            if not os.path.exists(input_dir):\n",
    "                print(f\"Models directory '{input_dir}' does not exist\")\n",
    "                return False\n",
    "                \n",
    "            # Try to load the classifier\n",
    "            classifier_path = f\"{input_dir}/outcome_classifier.pkl\"\n",
    "            if os.path.exists(classifier_path):\n",
    "                with open(classifier_path, 'rb') as f:\n",
    "                    self.classifier = pickle.load(f)\n",
    "            \n",
    "            # Try to load the regressor\n",
    "            regressor_path = f\"{input_dir}/time_regressor.pkl\"\n",
    "            if os.path.exists(regressor_path):\n",
    "                with open(regressor_path, 'rb') as f:\n",
    "                    self.regressor = pickle.load(f)\n",
    "            \n",
    "            # Try to load the cluster model\n",
    "            cluster_path = f\"{input_dir}/cluster_model.pkl\"\n",
    "            if os.path.exists(cluster_path):\n",
    "                with open(cluster_path, 'rb') as f:\n",
    "                    self.cluster_model = pickle.load(f)\n",
    "            \n",
    "            # Try to load the preprocessing pipeline\n",
    "            pipeline_path = f\"{input_dir}/preprocessing_pipeline.pkl\"\n",
    "            if os.path.exists(pipeline_path):\n",
    "                with open(pipeline_path, 'rb') as f:\n",
    "                    self.preprocessing_pipeline = pickle.load(f)\n",
    "            \n",
    "            print(f\"Models loaded from {input_dir}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'anonymized_hubspot_deals.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSalesPlaybookModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeals_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manonymized_hubspot_deals.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompanies_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manonymized_hubspot_companies.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtickets_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manonymized_hubspot_tickets.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmappings_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmappings.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Process and merge the data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mprocess_and_merge_data()\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mSalesPlaybookModel.__init__\u001b[0;34m(self, deals_path, companies_path, tickets_path, mappings_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mSalesPlaybookModel.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load CSV files\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeals \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeals_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompanies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompanies_path)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtickets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtickets_path)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'anonymized_hubspot_deals.csv'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the model\n",
    "    model = SalesPlaybookModel(\n",
    "        deals_path=\"anonymized_hubspot_deals.csv\",\n",
    "        companies_path=\"anonymized_hubspot_companies.csv\",\n",
    "        tickets_path=\"anonymized_hubspot_tickets.csv\",\n",
    "        mappings_path=\"mappings.json\"\n",
    "    )\n",
    "    \n",
    "    # Process and merge the data\n",
    "    model.process_and_merge_data()\n",
    "    \n",
    "    # Perform exploratory analysis\n",
    "    model.perform_exploratory_analysis(output_dir=\"analysis_output\")\n",
    "    \n",
    "    # Cluster companies\n",
    "    model.cluster_companies(n_clusters=5)\n",
    "    \n",
    "    # Train predictive models\n",
    "    model.train_deal_outcome_model()\n",
    "    model.train_time_to_close_model()\n",
    "    \n",
    "    # Save the trained models\n",
    "    model.save_models(output_dir=\"models\")\n",
    "    \n",
    "    # Example prediction for a new deal\n",
    "    sample_deal = {\n",
    "        'Amount': 50000,\n",
    "        'Deal probability': 0.7,\n",
    "        'Weighted amount': 35000,\n",
    "        'Forecast amount': 40000,\n",
    "        'Create Date': '2025-01-15',\n",
    "        'Pipeline': 'Sales Pipeline',\n",
    "        'Deal Stage': 'Negotiation',\n",
    "        'Forecast category': 'Best Case',\n",
    "        'ticket_count_per_deal': 2\n",
    "    }\n",
    "    \n",
    "    prediction = model.predict_new_deal(sample_deal)\n",
    "    print(\"\\nSample Deal Prediction:\")\n",
    "    print(f\"Win Probability: {prediction['win_probability']:.2%}\")\n",
    "    print(f\"Estimated Days to Close: {prediction['estimated_days_to_close']:.1f}\")\n",
    "    print(f\"Estimated Close Date: {prediction['estimated_close_date']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
